{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluzz — Détection de fraude bancaire  \n",
    "**Partie 3 — Augmentation de données avec SDV (Module 3)**\n",
    "\n",
    "Ce notebook utilise **SDV (Synthetic Data Vault)** pour générer des données synthétiques et équilibrer le dataset de fraude bancaire.\n",
    "\n",
    "**Objectifs :**\n",
    "- Gérer le déséquilibre des classes (0.173% de fraudes)\n",
    "- Générer des transactions frauduleuses synthétiques réalistes\n",
    "- Comparer les performances avant/après augmentation\n",
    "- Méthode de test rapide pour validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import SDV\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "print(\"Bibliothèques chargées avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset original\n",
    "df_original = pd.read_csv('../01_data/creditcard.csv')\n",
    "\n",
    "print(f\"Dataset original : {df_original.shape[0]} transactions\")\n",
    "print(f\"Répartition des classes :\")\n",
    "print(df_original['Class'].value_counts())\n",
    "print(f\"Taux de fraude : {df_original['Class'].mean()*100:.4f}%\")\n",
    "\n",
    "# Séparation des classes pour analyse\n",
    "df_legit = df_original[df_original['Class'] == 0].copy()\n",
    "df_fraud = df_original[df_original['Class'] == 1].copy()\n",
    "\n",
    "print(f\"\\nTransactions légitimes : {len(df_legit)}\")\n",
    "print(f\"Transactions frauduleuses : {len(df_fraud)}\")\n",
    "print(f\"Ratio déséquilibre : 1:{len(df_legit)//len(df_fraud)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration de SDV pour les données frauduleuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des métadonnées pour SDV\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(df_fraud)\n",
    "\n",
    "# Configuration du modèle SDV (Gaussian Copula pour données numériques)\n",
    "synthesizer = GaussianCopulaSynthesizer(\n",
    "    metadata=metadata,\n",
    "    default_distribution='gaussian_kde',  # Distribution adaptée aux données continues\n",
    "    numerical_distributions={\n",
    "        'Amount': 'gamma'  # Distribution gamma pour les montants (toujours positifs)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Métadonnées SDV configurées\")\n",
    "print(f\"Colonnes détectées : {list(metadata.columns.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle SDV sur les données frauduleuses\n",
    "print(\"Entraînement du modèle SDV sur les transactions frauduleuses...\")\n",
    "print(\"(Cela peut prendre quelques minutes)\")\n",
    "\n",
    "synthesizer.fit(df_fraud)\n",
    "\n",
    "print(\"✓ Modèle SDV entraîné avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Génération de données synthétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de fraudes à générer pour équilibrer\n",
    "num_legit = len(df_legit)\n",
    "num_fraud_original = len(df_fraud)\n",
    "\n",
    "# Stratégies d'équilibrage\n",
    "strategies = {\n",
    "    'equilibre_complet': num_legit - num_fraud_original,  # 50/50\n",
    "    'equilibre_partiel': int((num_legit * 0.1) - num_fraud_original),  # 10% de fraudes\n",
    "    'test_rapide': min(1000, num_legit - num_fraud_original)  # Pour tests rapides\n",
    "}\n",
    "\n",
    "print(\"Stratégies d'équilibrage disponibles :\")\n",
    "for strategy, count in strategies.items():\n",
    "    total_fraud = num_fraud_original + max(0, count)\n",
    "    ratio = total_fraud / (num_legit + total_fraud) * 100\n",
    "    print(f\"• {strategy}: +{max(0, count)} fraudes synthétiques → {ratio:.2f}% de fraudes\")\n",
    "\n",
    "# Sélection de la stratégie (changez ici selon vos besoins)\n",
    "STRATEGY = 'test_rapide'  # Changez en 'equilibre_partiel' ou 'equilibre_complet' si besoin\n",
    "num_synthetic_fraud = max(0, strategies[STRATEGY])\n",
    "\n",
    "print(f\"\\n🎯 Stratégie sélectionnée : {STRATEGY}\")\n",
    "print(f\"Génération de {num_synthetic_fraud} transactions frauduleuses synthétiques...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des données synthétiques\n",
    "if num_synthetic_fraud > 0:\n",
    "    synthetic_fraud = synthesizer.sample(num_rows=num_synthetic_fraud)\n",
    "    \n",
    "    # Vérification de la qualité des données générées\n",
    "    print(\"Données synthétiques générées :\")\n",
    "    print(f\"Forme : {synthetic_fraud.shape}\")\n",
    "    print(f\"Valeurs manquantes : {synthetic_fraud.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Affichage des statistiques comparatives\n",
    "    print(\"\\nComparaison statistiques (Amount) :\")\n",
    "    print(f\"Fraudes originales - Moyenne: {df_fraud['Amount'].mean():.2f}, Std: {df_fraud['Amount'].std():.2f}\")\n",
    "    print(f\"Fraudes synthétiques - Moyenne: {synthetic_fraud['Amount'].mean():.2f}, Std: {synthetic_fraud['Amount'].std():.2f}\")\n",
    "    \n",
    "    # Création du dataset augmenté\n",
    "    df_augmented = pd.concat([\n",
    "        df_original,  # Données originales\n",
    "        synthetic_fraud  # Fraudes synthétiques\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Dataset augmenté créé : {df_augmented.shape[0]} transactions\")\n",
    "    print(f\"Nouveau taux de fraude : {df_augmented['Class'].mean()*100:.4f}%\")\n",
    "    \n",
    "else:\n",
    "    df_augmented = df_original.copy()\n",
    "    print(\"Aucune donnée synthétique générée (stratégie ne le nécessite pas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation de la distribution des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des distributions avant/après augmentation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution des classes - avant\n",
    "df_original['Class'].value_counts().plot(kind='bar', ax=axes[0,0], color=['green', 'red'], alpha=0.7)\n",
    "axes[0,0].set_title('Distribution avant augmentation', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Classe (0=Légit, 1=Fraude)')\n",
    "axes[0,0].set_ylabel('Nombre de transactions')\n",
    "\n",
    "# Distribution des classes - après\n",
    "df_augmented['Class'].value_counts().plot(kind='bar', ax=axes[0,1], color=['green', 'red'], alpha=0.7)\n",
    "axes[0,1].set_title('Distribution après augmentation', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Classe (0=Légit, 1=Fraude)')\n",
    "axes[0,1].set_ylabel('Nombre de transactions')\n",
    "\n",
    "# Distribution des montants - fraudes originales vs synthétiques\n",
    "if num_synthetic_fraud > 0:\n",
    "    axes[1,0].hist(df_fraud['Amount'], bins=50, alpha=0.7, label='Fraudes originales', color='red')\n",
    "    axes[1,0].hist(synthetic_fraud['Amount'], bins=50, alpha=0.7, label='Fraudes synthétiques', color='orange')\n",
    "    axes[1,0].set_title('Distribution des montants - Fraudes', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Montant')\n",
    "    axes[1,0].set_ylabel('Fréquence')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].set_yscale('log')\n",
    "\n",
    "# Comparaison des taux de fraude\n",
    "rates = [\n",
    "    df_original['Class'].mean() * 100,\n",
    "    df_augmented['Class'].mean() * 100\n",
    "]\n",
    "axes[1,1].bar(['Avant', 'Après'], rates, color=['lightcoral', 'darkred'], alpha=0.7)\n",
    "axes[1,1].set_title('Taux de fraude (%)', fontweight='bold')\n",
    "axes[1,1].set_ylabel('Pourcentage')\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, rate in enumerate(rates):\n",
    "    axes[1,1].text(i, rate + 0.1, f'{rate:.3f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test rapide de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de test rapide pour comparer les performances\n",
    "def quick_performance_test(df, test_name, sample_size=10000):\n",
    "    \"\"\"Test rapide avec échantillonnage pour comparer les performances\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== {test_name} ===\")\n",
    "    \n",
    "    # Échantillonnage stratifié pour test rapide\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.groupby('Class', group_keys=False).apply(\n",
    "            lambda x: x.sample(min(len(x), sample_size//2), random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "        print(f\"Échantillon de test : {len(df_sample)} transactions\")\n",
    "    else:\n",
    "        df_sample = df.copy()\n",
    "        print(f\"Dataset complet utilisé : {len(df_sample)} transactions\")\n",
    "    \n",
    "    # Séparation train/test\n",
    "    X = df_sample.drop('Class', axis=1)\n",
    "    y = df_sample['Class']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Modèle simple pour test rapide\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=50,  # Réduit pour rapidité\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Métriques\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"F1-Score : {f1:.4f}\")\n",
    "    print(f\"Précision : {precision:.4f}\")\n",
    "    print(f\"Rappel : {recall:.4f}\")\n",
    "    print(f\"Fraudes en test : {y_test.sum()} ({y_test.mean()*100:.3f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'test_name': test_name,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'fraud_rate': y_test.mean() * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests de performance rapides\n",
    "results = []\n",
    "\n",
    "# Test sur données originales\n",
    "result_original = quick_performance_test(df_original, \"Dataset Original\")\n",
    "results.append(result_original)\n",
    "\n",
    "# Test sur données augmentées (si différentes)\n",
    "if len(df_augmented) != len(df_original):\n",
    "    result_augmented = quick_performance_test(df_augmented, \"Dataset Augmenté\")\n",
    "    results.append(result_augmented)\n",
    "\n",
    "# Comparaison des résultats\n",
    "if len(results) > 1:\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARAISON DES PERFORMANCES\")\n",
    "    print(\"=\"*80)\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Calcul des améliorations\n",
    "    f1_improvement = (results[1]['f1_score'] - results[0]['f1_score']) / results[0]['f1_score'] * 100\n",
    "    precision_improvement = (results[1]['precision'] - results[0]['precision']) / results[0]['precision'] * 100\n",
    "    recall_improvement = (results[1]['recall'] - results[0]['recall']) / results[0]['recall'] * 100\n",
    "    \n",
    "    print(f\"\\n📈 Améliorations avec augmentation SDV :\")\n",
    "    print(f\"F1-Score : {f1_improvement:+.2f}%\")\n",
    "    print(f\"Précision : {precision_improvement:+.2f}%\")\n",
    "    print(f\"Rappel : {recall_improvement:+.2f}%\")\nelse:\n",
    "    print(\"\\n📝 Test effectué sur dataset original uniquement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde du dataset augmenté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataset augmenté pour utilisation ultérieure\n",
    "if len(df_augmented) != len(df_original):\n",
    "    output_path = '../01_data/creditcard_augmented.csv'\n",
    "    df_augmented.to_csv(output_path, index=False)\n",
    "    print(f\"✓ Dataset augmenté sauvegardé : {output_path}\")\n",
    "    print(f\"Taille : {df_augmented.shape[0]} transactions\")\n",
    "    print(f\"Taux de fraude : {df_augmented['Class'].mean()*100:.4f}%\")\nelse:\n",
    "    print(\"Aucune sauvegarde nécessaire (pas d'augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion et recommandations\n",
    "\n",
    "### Résultats de l'augmentation SDV\n",
    "L'augmentation de données avec SDV permet de :\n",
    "- **Équilibrer les classes** pour améliorer l'apprentissage\n",
    "- **Générer des fraudes réalistes** basées sur les patterns existants\n",
    "- **Améliorer les métriques** de détection (F1, Précision, Rappel)\n",
    "\n",
    "### Stratégies disponibles :\n",
    "1. **test_rapide** : Génération limitée pour tests et prototypage\n",
    "2. **equilibre_partiel** : 10% de fraudes (plus réaliste)\n",
    "3. **equilibre_complet** : 50/50 (équilibrage total)\n",
    "\n",
    "### Recommandations :\n",
    "- Utiliser **test_rapide** pour les expérimentations\n",
    "- Utiliser **equilibre_partiel** pour l'entraînement final\n",
    "- Valider sur données réelles non augmentées\n",
    "- Surveiller la qualité des données synthétiques\n",
    "\n",
    "### Points d'attention :\n",
    "- Les données synthétiques ne remplacent pas les vraies données\n",
    "- Toujours valider les performances sur un jeu de test réel\n",
    "- Surveiller le sur-apprentissage avec les données augmentées"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}