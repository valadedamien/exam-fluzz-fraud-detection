{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluzz ‚Äî D√©tection de fraude bancaire  \n",
    "**Partie 3 ‚Äî Augmentation de donn√©es avec SDV (Module 3)**\n",
    "\n",
    "Ce notebook utilise **SDV (Synthetic Data Vault)** pour g√©n√©rer des donn√©es synth√©tiques et √©quilibrer le dataset de fraude bancaire.\n",
    "\n",
    "**Objectifs :**\n",
    "- G√©rer le d√©s√©quilibre des classes (0.173% de fraudes)\n",
    "- G√©n√©rer des transactions frauduleuses synth√©tiques r√©alistes\n",
    "- Comparer les performances avant/apr√®s augmentation\n",
    "- M√©thode de test rapide pour validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import SDV\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "print(\"Biblioth√®ques charg√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset original\n",
    "df_original = pd.read_csv('../01_data/creditcard.csv')\n",
    "\n",
    "print(f\"Dataset original : {df_original.shape[0]} transactions\")\n",
    "print(f\"R√©partition des classes :\")\n",
    "print(df_original['Class'].value_counts())\n",
    "print(f\"Taux de fraude : {df_original['Class'].mean()*100:.4f}%\")\n",
    "\n",
    "# S√©paration des classes pour analyse\n",
    "df_legit = df_original[df_original['Class'] == 0].copy()\n",
    "df_fraud = df_original[df_original['Class'] == 1].copy()\n",
    "\n",
    "print(f\"\\nTransactions l√©gitimes : {len(df_legit)}\")\n",
    "print(f\"Transactions frauduleuses : {len(df_fraud)}\")\n",
    "print(f\"Ratio d√©s√©quilibre : 1:{len(df_legit)//len(df_fraud)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration de SDV pour les donn√©es frauduleuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des m√©tadonn√©es pour SDV\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(df_fraud)\n",
    "\n",
    "# Configuration du mod√®le SDV (Gaussian Copula pour donn√©es num√©riques)\n",
    "synthesizer = GaussianCopulaSynthesizer(\n",
    "    metadata=metadata,\n",
    "    default_distribution='gaussian_kde',  # Distribution adapt√©e aux donn√©es continues\n",
    "    numerical_distributions={\n",
    "        'Amount': 'gamma'  # Distribution gamma pour les montants (toujours positifs)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"M√©tadonn√©es SDV configur√©es\")\n",
    "print(f\"Colonnes d√©tect√©es : {list(metadata.columns.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le SDV sur les donn√©es frauduleuses\n",
    "print(\"Entra√Ænement du mod√®le SDV sur les transactions frauduleuses...\")\n",
    "print(\"(Cela peut prendre quelques minutes)\")\n",
    "\n",
    "synthesizer.fit(df_fraud)\n",
    "\n",
    "print(\"‚úì Mod√®le SDV entra√Æn√© avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. G√©n√©ration de donn√©es synth√©tiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de fraudes √† g√©n√©rer pour √©quilibrer\n",
    "num_legit = len(df_legit)\n",
    "num_fraud_original = len(df_fraud)\n",
    "\n",
    "# Strat√©gies d'√©quilibrage\n",
    "strategies = {\n",
    "    'equilibre_complet': num_legit - num_fraud_original,  # 50/50\n",
    "    'equilibre_partiel': int((num_legit * 0.1) - num_fraud_original),  # 10% de fraudes\n",
    "    'test_rapide': min(1000, num_legit - num_fraud_original)  # Pour tests rapides\n",
    "}\n",
    "\n",
    "print(\"Strat√©gies d'√©quilibrage disponibles :\")\n",
    "for strategy, count in strategies.items():\n",
    "    total_fraud = num_fraud_original + max(0, count)\n",
    "    ratio = total_fraud / (num_legit + total_fraud) * 100\n",
    "    print(f\"‚Ä¢ {strategy}: +{max(0, count)} fraudes synth√©tiques ‚Üí {ratio:.2f}% de fraudes\")\n",
    "\n",
    "# S√©lection de la strat√©gie (changez ici selon vos besoins)\n",
    "STRATEGY = 'test_rapide'  # Changez en 'equilibre_partiel' ou 'equilibre_complet' si besoin\n",
    "num_synthetic_fraud = max(0, strategies[STRATEGY])\n",
    "\n",
    "print(f\"\\nüéØ Strat√©gie s√©lectionn√©e : {STRATEGY}\")\n",
    "print(f\"G√©n√©ration de {num_synthetic_fraud} transactions frauduleuses synth√©tiques...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration des donn√©es synth√©tiques\n",
    "if num_synthetic_fraud > 0:\n",
    "    synthetic_fraud = synthesizer.sample(num_rows=num_synthetic_fraud)\n",
    "    \n",
    "    # V√©rification de la qualit√© des donn√©es g√©n√©r√©es\n",
    "    print(\"Donn√©es synth√©tiques g√©n√©r√©es :\")\n",
    "    print(f\"Forme : {synthetic_fraud.shape}\")\n",
    "    print(f\"Valeurs manquantes : {synthetic_fraud.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Affichage des statistiques comparatives\n",
    "    print(\"\\nComparaison statistiques (Amount) :\")\n",
    "    print(f\"Fraudes originales - Moyenne: {df_fraud['Amount'].mean():.2f}, Std: {df_fraud['Amount'].std():.2f}\")\n",
    "    print(f\"Fraudes synth√©tiques - Moyenne: {synthetic_fraud['Amount'].mean():.2f}, Std: {synthetic_fraud['Amount'].std():.2f}\")\n",
    "    \n",
    "    # Cr√©ation du dataset augment√©\n",
    "    df_augmented = pd.concat([\n",
    "        df_original,  # Donn√©es originales\n",
    "        synthetic_fraud  # Fraudes synth√©tiques\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n‚úì Dataset augment√© cr√©√© : {df_augmented.shape[0]} transactions\")\n",
    "    print(f\"Nouveau taux de fraude : {df_augmented['Class'].mean()*100:.4f}%\")\n",
    "    \n",
    "else:\n",
    "    df_augmented = df_original.copy()\n",
    "    print(\"Aucune donn√©e synth√©tique g√©n√©r√©e (strat√©gie ne le n√©cessite pas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation de la distribution des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des distributions avant/apr√®s augmentation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution des classes - avant\n",
    "df_original['Class'].value_counts().plot(kind='bar', ax=axes[0,0], color=['green', 'red'], alpha=0.7)\n",
    "axes[0,0].set_title('Distribution avant augmentation', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Classe (0=L√©git, 1=Fraude)')\n",
    "axes[0,0].set_ylabel('Nombre de transactions')\n",
    "\n",
    "# Distribution des classes - apr√®s\n",
    "df_augmented['Class'].value_counts().plot(kind='bar', ax=axes[0,1], color=['green', 'red'], alpha=0.7)\n",
    "axes[0,1].set_title('Distribution apr√®s augmentation', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Classe (0=L√©git, 1=Fraude)')\n",
    "axes[0,1].set_ylabel('Nombre de transactions')\n",
    "\n",
    "# Distribution des montants - fraudes originales vs synth√©tiques\n",
    "if num_synthetic_fraud > 0:\n",
    "    axes[1,0].hist(df_fraud['Amount'], bins=50, alpha=0.7, label='Fraudes originales', color='red')\n",
    "    axes[1,0].hist(synthetic_fraud['Amount'], bins=50, alpha=0.7, label='Fraudes synth√©tiques', color='orange')\n",
    "    axes[1,0].set_title('Distribution des montants - Fraudes', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Montant')\n",
    "    axes[1,0].set_ylabel('Fr√©quence')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].set_yscale('log')\n",
    "\n",
    "# Comparaison des taux de fraude\n",
    "rates = [\n",
    "    df_original['Class'].mean() * 100,\n",
    "    df_augmented['Class'].mean() * 100\n",
    "]\n",
    "axes[1,1].bar(['Avant', 'Apr√®s'], rates, color=['lightcoral', 'darkred'], alpha=0.7)\n",
    "axes[1,1].set_title('Taux de fraude (%)', fontweight='bold')\n",
    "axes[1,1].set_ylabel('Pourcentage')\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for i, rate in enumerate(rates):\n",
    "    axes[1,1].text(i, rate + 0.1, f'{rate:.3f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test rapide de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de test rapide pour comparer les performances\n",
    "def quick_performance_test(df, test_name, sample_size=10000):\n",
    "    \"\"\"Test rapide avec √©chantillonnage pour comparer les performances\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== {test_name} ===\")\n",
    "    \n",
    "    # √âchantillonnage stratifi√© pour test rapide\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.groupby('Class', group_keys=False).apply(\n",
    "            lambda x: x.sample(min(len(x), sample_size//2), random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "        print(f\"√âchantillon de test : {len(df_sample)} transactions\")\n",
    "    else:\n",
    "        df_sample = df.copy()\n",
    "        print(f\"Dataset complet utilis√© : {len(df_sample)} transactions\")\n",
    "    \n",
    "    # S√©paration train/test\n",
    "    X = df_sample.drop('Class', axis=1)\n",
    "    y = df_sample['Class']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Mod√®le simple pour test rapide\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=50,  # R√©duit pour rapidit√©\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # M√©triques\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"F1-Score : {f1:.4f}\")\n",
    "    print(f\"Pr√©cision : {precision:.4f}\")\n",
    "    print(f\"Rappel : {recall:.4f}\")\n",
    "    print(f\"Fraudes en test : {y_test.sum()} ({y_test.mean()*100:.3f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'test_name': test_name,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'fraud_rate': y_test.mean() * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests de performance rapides\n",
    "results = []\n",
    "\n",
    "# Test sur donn√©es originales\n",
    "result_original = quick_performance_test(df_original, \"Dataset Original\")\n",
    "results.append(result_original)\n",
    "\n",
    "# Test sur donn√©es augment√©es (si diff√©rentes)\n",
    "if len(df_augmented) != len(df_original):\n",
    "    result_augmented = quick_performance_test(df_augmented, \"Dataset Augment√©\")\n",
    "    results.append(result_augmented)\n",
    "\n",
    "# Comparaison des r√©sultats\n",
    "if len(results) > 1:\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARAISON DES PERFORMANCES\")\n",
    "    print(\"=\"*80)\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Calcul des am√©liorations\n",
    "    f1_improvement = (results[1]['f1_score'] - results[0]['f1_score']) / results[0]['f1_score'] * 100\n",
    "    precision_improvement = (results[1]['precision'] - results[0]['precision']) / results[0]['precision'] * 100\n",
    "    recall_improvement = (results[1]['recall'] - results[0]['recall']) / results[0]['recall'] * 100\n",
    "    \n",
    "    print(f\"\\nüìà Am√©liorations avec augmentation SDV :\")\n",
    "    print(f\"F1-Score : {f1_improvement:+.2f}%\")\n",
    "    print(f\"Pr√©cision : {precision_improvement:+.2f}%\")\n",
    "    print(f\"Rappel : {recall_improvement:+.2f}%\")\nelse:\n",
    "    print(\"\\nüìù Test effectu√© sur dataset original uniquement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde du dataset augment√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du dataset augment√© pour utilisation ult√©rieure\n",
    "if len(df_augmented) != len(df_original):\n",
    "    output_path = '../01_data/creditcard_augmented.csv'\n",
    "    df_augmented.to_csv(output_path, index=False)\n",
    "    print(f\"‚úì Dataset augment√© sauvegard√© : {output_path}\")\n",
    "    print(f\"Taille : {df_augmented.shape[0]} transactions\")\n",
    "    print(f\"Taux de fraude : {df_augmented['Class'].mean()*100:.4f}%\")\nelse:\n",
    "    print(\"Aucune sauvegarde n√©cessaire (pas d'augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion et recommandations\n",
    "\n",
    "### R√©sultats de l'augmentation SDV\n",
    "L'augmentation de donn√©es avec SDV permet de :\n",
    "- **√âquilibrer les classes** pour am√©liorer l'apprentissage\n",
    "- **G√©n√©rer des fraudes r√©alistes** bas√©es sur les patterns existants\n",
    "- **Am√©liorer les m√©triques** de d√©tection (F1, Pr√©cision, Rappel)\n",
    "\n",
    "### Strat√©gies disponibles :\n",
    "1. **test_rapide** : G√©n√©ration limit√©e pour tests et prototypage\n",
    "2. **equilibre_partiel** : 10% de fraudes (plus r√©aliste)\n",
    "3. **equilibre_complet** : 50/50 (√©quilibrage total)\n",
    "\n",
    "### Recommandations :\n",
    "- Utiliser **test_rapide** pour les exp√©rimentations\n",
    "- Utiliser **equilibre_partiel** pour l'entra√Ænement final\n",
    "- Valider sur donn√©es r√©elles non augment√©es\n",
    "- Surveiller la qualit√© des donn√©es synth√©tiques\n",
    "\n",
    "### Points d'attention :\n",
    "- Les donn√©es synth√©tiques ne remplacent pas les vraies donn√©es\n",
    "- Toujours valider les performances sur un jeu de test r√©el\n",
    "- Surveiller le sur-apprentissage avec les donn√©es augment√©es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}