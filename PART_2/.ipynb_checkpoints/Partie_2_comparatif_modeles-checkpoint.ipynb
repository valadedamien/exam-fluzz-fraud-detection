{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fluzz — Détection de fraude bancaire  \n**Partie 2 — Comparatif des modèles (Module 2)**\n\nCe notebook compare 3 algorithmes d'apprentissage automatique pour la détection de fraude bancaire :\n- **Régression Logistique** : modèle linéaire simple et interprétable\n- **Random Forest** : ensemble d'arbres de décision\n- **MLP** (Multi-Layer Perceptron) : réseau de neurones\n\n**Métriques d'évaluation** : F1-Score, Précision, Rappel (adaptées aux classes déséquilibrées)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données et préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Chargement des données\n",
    "df = pd.read_csv('../01_data/creditcard.csv')\n",
    "print(f\"Dataset chargé : {df.shape[0]} transactions, {df.shape[1]} features\")\n",
    "print(f\"Taux de fraude : {df['Class'].mean()*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des features et de la cible\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Jeu d'entraînement : {X_train.shape[0]} échantillons\")\n",
    "print(f\"Jeu de test : {X_test.shape[0]} échantillons\")\n",
    "print(f\"Fraudes dans le test : {y_test.sum()} ({y_test.mean()*100:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des données (nécessaire pour MLP et Régression Logistique)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Données normalisées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entraînement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des modèles avec gestion du déséquilibre\n",
    "models = {\n",
    "    'Régression Logistique': LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    ),\n",
    "    'MLP': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        random_state=42,\n",
    "        max_iter=500,\n",
    "        early_stopping=True\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Modèles configurés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement des modèles\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Début de l'entraînement...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Entraînement de {name}...\")\n",
    "    \n",
    "    # Utiliser les données normalisées pour LR et MLP, originales pour RF\n",
    "    if name in ['Régression Logistique', 'MLP']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    print(f\"✓ {name} entraîné\")\n",
    "\n",
    "print(\"\\nTous les modèles sont entraînés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Évaluation et comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des modèles\n",
    "results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Prédictions\n",
    "    if name in ['Régression Logistique', 'MLP']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Modèle': name,\n",
    "        'F1-Score': f1,\n",
    "        'Précision': precision,\n",
    "        'Rappel': recall\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"F1-Score : {f1:.4f}\")\n",
    "    print(f\"Précision : {precision:.4f}\")\n",
    "    print(f\"Rappel : {recall:.4f}\")\n",
    "\n",
    "# Création du DataFrame des résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RÉSUMÉ DES PERFORMANCES\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['F1-Score', 'Précision', 'Rappel']\n",
    "colors = ['#2E8B57', '#4169E1', '#DC143C']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    bars = axes[i].bar(results_df['Modèle'], results_df[metric], \n",
    "                       color=colors[i], alpha=0.7)\n",
    "    axes[i].set_title(f'{metric}', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for bar, value in zip(bars, results_df[metric]):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                     f'{value:.3f}', ha='center', va='bottom', \n",
    "                     fontweight='bold')\n",
    "    \n",
    "    # Rotation des labels\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparaison des Modèles de Détection de Fraude', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse détaillée du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification du meilleur modèle basé sur le F1-Score\n",
    "best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Modèle']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Meilleur modèle : {best_model_name}\")\n",
    "print(f\"F1-Score : {results_df.loc[results_df['F1-Score'].idxmax(), 'F1-Score']:.4f}\")\n",
    "\n",
    "# Prédictions du meilleur modèle\n",
    "if best_model_name in ['Régression Logistique', 'MLP']:\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Rapport de classification détaillé\n",
    "print(\"\\nRapport de classification détaillé :\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Non Fraude', 'Fraude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non Fraude', 'Fraude'],\n",
    "            yticklabels=['Non Fraude', 'Fraude'])\n",
    "plt.title(f'Matrice de Confusion - {best_model_name}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Prédictions', fontsize=12)\n",
    "plt.ylabel('Vraies valeurs', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Calcul des taux d'erreur\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nAnalyse des erreurs :\")\n",
    "print(f\"Vrais positifs (fraudes détectées) : {tp}\")\n",
    "print(f\"Faux positifs (fausses alertes) : {fp}\")\n",
    "print(f\"Faux négatifs (fraudes manquées) : {fn}\")\n",
    "print(f\"Vrais négatifs (transactions légitimes) : {tn}\")\n",
    "print(f\"\\nTaux de fausses alertes : {fp/(fp+tn)*100:.2f}%\")\n",
    "print(f\"Taux de fraudes manquées : {fn/(fn+tp)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "### Résumé des performances\n",
    "Les résultats montrent les performances relatives de chaque algorithme sur notre dataset de fraude bancaire.\n",
    "\n",
    "### Points clés à retenir :\n",
    "- **F1-Score** : mesure équilibrée entre précision et rappel\n",
    "- **Précision** : pourcentage de fraudes correctement identifiées parmi les alertes\n",
    "- **Rappel** : pourcentage de fraudes détectées parmi toutes les fraudes réelles\n",
    "\n",
    "### Recommandations :\n",
    "- Pour minimiser les fausses alertes → privilégier la **précision**\n",
    "- Pour ne manquer aucune fraude → privilégier le **rappel**\n",
    "- Pour un équilibre optimal → privilégier le **F1-Score**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}